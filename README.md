# Awesome Deep Research Agents & Benchmarks [![Awesome](https://awesome.re/badge-flat.svg)](https://awesome.re)

> A curated list of advanced AI research agents, benchmarks, and evaluation frameworks.
>
> Contribute and explore cutting-edge tools for AI research.

Welcome to **Awesome Deep Research Agents & Benchmarks**! This list includes the current research agents, benchmarks, and evaluation frameworks designed to push the boundaries of artificial intelligence. If you're developing a research agent or benchmarking framework, feel free to contribute.

Stay updated with the latest AI research tools and methodologies. Want to feature your project? Submit a PR!

## Contents

- [üåü Editor's Choice](#editors-choice)
- [ü§ñ AI Research Agents](#ai-research-agents)
- [üìä Benchmarks & Evaluation](#benchmarks-evaluation)
- [üîç Datasets](#datasets)
- [üìù Papers & Learning Resources](#papers-learning-resources)

---

## Editor's Choice

- [GAIA: A Benchmark for General AI Assistants](https://arxiv.org/abs/2311.12983) ‚Äì Introduces GAIA, a benchmark proposing real-world questions that require fundamental abilities such as reasoning, multi-modality handling, web browsing, and tool-use proficiency.

- [Manus: China's Autonomous AI Agent](https://www.forbes.com/sites/craigsmith/2025/03/08/chinas-autonomous-agent-manus-changes-everything/) ‚Äì An overview of Manus, an autonomous AI agent developed by the Chinese startup Monica, capable of independently carrying out complex real-world tasks without direct or continuous human guidance.

- [HELM: Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110) ‚Äì A comprehensive framework developed by Stanford for evaluating language models across various dimensions, including accuracy, robustness, and fairness.

- [BIG-bench: A Benchmark for Language Models](https://github.com/google/BIG-bench) ‚Äì A collaborative benchmark initiative by Google, comprising a suite of challenging tasks designed to measure the performance of language models.

- [AGIEval: A Human-Centric Benchmark for Evaluating General AI Capabilities]([https://github.com/microsoft/AGIEval](https://arxiv.org/abs/2304.06364)) ‚Äì A benchmark developed by Microsoft to evaluate general AI capabilities with a focus on human-centric tasks.


## AI Research Agents

### Autonomous Research Agents

- [AutoGPT](https://github.com/Torantulino/Auto-GPT) - An experimental open-source application showcasing GPT-4 running fully autonomously.
- [BabyAGI](https://github.com/yoheinakajima/babyagi) - A simple task management system powered by GPT-based agents.
- [CAMEL](https://github.com/lightaime/camel) - Cooperative AI agents working on research-driven tasks.
- [MetaGPT](https://github.com/geekan/MetaGPT) - Multi-agent framework for collaborative AI research.

### Specialized AI Research Assistants

- [Elicit](https://elicit.org/) - AI-powered literature review and research automation.
- [PaperQA](https://github.com/whitead/paper-qa) - AI-driven paper-based Q&A system for research.
- [ResearchRabbit](https://www.researchrabbit.ai/) - AI tool for discovering and tracking academic papers.

## Benchmarks & Evaluation

- [GAIA Benchmark](https://gaia.ai/research) - Evaluates multi-modal AI capabilities in reasoning and experimentation.
- [HELM (Holistic Evaluation of Language Models)](https://crfm.stanford.edu/helm/) - Comprehensive LLM benchmarking framework.
- [ARC (AI2 Reasoning Challenge)](https://allenai.org/data/arc) - Benchmark for measuring machine reasoning capabilities.
- [SuperGLUE](https://super.gluebenchmark.com/) - Benchmark designed for evaluating LLM generalization.
- [MMLU (Massive Multitask Language Understanding)](https://github.com/hendrycks/test) - A diverse benchmark to measure AI understanding across multiple domains.
- [BIG-bench](https://github.com/google/BIG-bench) - A collection of diverse, human-designed tasks to evaluate language models.
- [AGIEval](https://github.com/microsoft/AGIEval) - Benchmark suite for evaluating the performance of AI agents in general intelligence tasks.
- [MLCommons Benchmarks](https://mlcommons.org/en/) - A suite of ML performance benchmarks.

## Datasets

- [PapersWithCode](https://paperswithcode.com/) - A platform offering state-of-the-art results, benchmarks, and datasets.
- [The Pile](https://pile.eleuther.ai/) - A high-quality dataset designed for large-scale language model training.
- [Hugging Face Datasets](https://huggingface.co/datasets) - A collection of diverse datasets for AI research.
- [AllenAI Datasets](https://allenai.org/data) - AI2 datasets for NLP and machine reasoning.
- [MATH Dataset](https://arxiv.org/abs/2302.01318) - A benchmark for evaluating AI mathematical reasoning.

---

### Contributions

We welcome contributions! If you know of a tool, dataset, or research agent that belongs on this list, feel free to submit a PR.

### License

[Apache License 2.0](LICENSE)

